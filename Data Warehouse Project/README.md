# Project Data Warehouse
## Project Overview

This project captures the development of an ETL pipeline to build a data warehouses hosted on AMAZON Redshift. 
Sparkify, a music streaming startup with a fast growing user base and song database plans to move their processes and data onto the cloud. Currently, their data resides in S3, JSON directory with logs on user activity on the app, as well as a  JSON metadata directory with data on the songs in their app.



## Dataset 
This project consist of two datasets residing in AMAZON S3. 

#### Song Dataset: 
Located in [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/), with each file in JSON format containing metadata about a song and the artist of that song and partitioned by the first three letters of each song's track ID.

Sample:
```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

## Log Dataset
This consists of JSON log files partition by year and month, generated by an event simulator -  [event simulator](https://github.com/Interana/eventsim) using the  on songs from the songs dataset, simulating app activity logs from an imaginary users streaming music on the app based.

Sample: 

    {"artist":null,"auth":"Logged In","firstName":"Walter","gender":"M","itemInSession":0,"lastName":"Frye","length":null,"level":"free","location":"San Francisco-Oakland-Hayward, CA","method":"GET","page":"Home","registration":1540919166796.0,"sessionId":38,"song":null,"status":200,"ts":1541105830796,"userAgent":"\"Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/36.0.1985.143 Safari\/537.36\"","userId":"39"}


## Schema 
This projects employs the Star Schemas with several dimensional tables and a fact table. Also on S3, we wil be extracting the data from the JSON format first to a staging Database [staging_events and staging_songs] for transformation before it is loaded into the AMAZON RedShift Cluster Database. 

#### Fact Table
songplays - records in event data associated with song plays. Columns for the table:

    songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

#### Dimension Tables 

##### users

    user_id, first_name, last_name, gender, level
##### songs

    song_id, title, artist_id, year, duration

##### artists

    artist_id, name, location, lattitude, longitude

##### time

    start_time, hour, day, week, month, year, weekday


## How to Run
#### First create a CLuster on AMAZON Redshift, taking note of the various details such as HOST[endpoint], database name, database user, password and port-number.

#### Secondly, create an IAM role with S3 read access only, taking note of the key and secret

#### Setup Configurations 
Utilising the info obtained from the first and second steps above, input the necessary data into the right variable to Setup the dwh.cfg file . As seen below;

```
[CLUSTER]
HOST=''
DB_NAME=''
DB_USER=''
DB_PASSWORD=''
DB_PORT=5439

[IAM_ROLE]
ARN=<IAM Role arn>

[S3]
LOG_DATA='s3://udacity-dend/log_data'
LOG_JSONPATH='s3://udacity-dend/log_json_path.json'
SONG_DATA='s3://udacity-dend/song_data'

```

#### Develop the necessary queries to develop the tables 

    $ python sql_queries.py 
    
#### Create tables using the create tables script

    $ python create_tables.py


#### Load Data
    
    $ Run the etl.py script 

Reference: [AWS Redshift Doc](https://aws.amazon.com/redshift/getting-started/?p=rs&bttn=hero&exp=b)